{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model3 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes with same size/radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically all different shapes with the same size/radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from CNNcount import shape_images as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed\n",
    "diff_shapes_same_radius_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/diff_shapes_same_radius_60k.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train, test and validaiton subsets\n",
    "if os.path.isfile(diff_shapes_same_radius_fname): # already generated- just load\n",
    "    print (\"The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = si.load_split_data(diff_shapes_same_radius_fname)    \n",
    "    \n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of different shapes (circle, square, diamond) with same radius does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAD7CAYAAADEpDe3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADGpJREFUeJzt3U+IXeUZgPHnFYliDCTTGZGkJLcSEASDC5cuXBhwpZukiBZsQII7XXRZdKF0XSxC1irYQlZ1IYKIoItBZhBcRFyIM1IS0eBE69/Q9usiMzqZyczcO3O/9/x7fnDp0MbrN2+Pzz2e+517o5SCJKmum5pegCQNgbGVpATGVpISGFtJSmBsJSmBsZWkBMZWkhJ0OrYRsS8izkfEUkSUiHiw6TX1ifOtJyLuiYiFiFhZfbwdEfc0va4+aOtx2+nYrnof+APwRdML6SnnW8dF4BQwA8wC/wT+3uiK+qV1x22jsV195flTRHwUEd9ExD8i4tZx//pSytVSyl9LKe8D/6241E5yvvVMYbZXSilL5dotnMG1+R6vtuAO6etx24Yz298DDwO/A04Af4yIoxFxZZvH480uuVOcbz17nm1EXAF+Av4G/CX9N2iv3h23Nze9AOClUspFgIh4A7ivlHIOONjssnrD+daz59mWUg5GxH7gSWC5zjI7qXfHbRvObNdfU/kBuL2phfSU861nKrMtpXwPnANeiYg7prGwHujdcduG2G6y+q8L323zeKLpNXaZ861nD7O9CbgNOJK43E7p+nHbhssIm5RSPmfMV7KIuIVrbzAA7Fu9kP5z8bMjt+R86xl3thFxErgMfATsB14EVoCPqy6ww7p+3LbyzHZCnwA/cu2M4K3Vn481uqJ+cb51HAReB74BPuXaToSHSyk/Nbqq/mjdcRueoEhSfX04s5Wk1jO2kpTA2EpSAmMrSQmMrSQlmGif7ezsbBmNRpWW0n6Li4uXSylzNZ7b2TrbWmrOFpzvuPOdKLaj0YiFhYXdr6rjIqLavevO1tnWUnO24HzHna+XESQpgbGVpATGVpISGFtJSmBsJSmBsZWkBMZWkhIYW0lKYGwlKYGxlaQExlaSEhhbSUpgbCUpgbGVKouInf+Qes/YShWthdbgythKlWwMrMEdNmMrVbBVWA3ucBlbacp2CqrBHSZjK01ZKWVP/7v6ydhKFWwVVEM7XMZWqmRjWA3tsBlbqaK1wBra6w3xurWxlSoztNcb6t5jYyspzZD3HhtbSSmGvvfY2Eqqzr3HxlZSAvceG1tJSYa+99jYSkoz5L3HxlZSqqHuPTa2ktINLbRgbCUphbGVpATGVpIS3Nz0AmoYZ4P0EK8ZSWpOL2MrNc0XfG3kZQRJSmBsJSmBsZWkBMZWkhIYW0lKYGwlKYGxlfZoCB98rb3r5T5b9y8qw/rIrv3ssaet9DK2Um3bfZ9WKcXoahMvI0hSAmMrTcgvL9RuGNuO8x/sfH55oXbDa7Yd5ZszUrd4ZttB2705oxxD/6ZYTc4zW2mX1sK6tgNB2o5nth3jmzPtY2g1DmPbMb45I3WTsZWkBMa2g3xzRuoe3yDrKN+ckbrFM9uOM7RSNxhbSUpgbCUpgbGVpASdju3Vq1c5deoUo9GIiODdd99tekm94nzrcbb1zM/Pc/LkSWZmZpibm+P06dNcunSp6WV1O7YADzzwAK+99hp33nln00vpJedbj7OtY2VlhbNnz7K0tMTy8jIHDhzgzJkzTS+LmOTd7Ij4Clie4t//XuBL4DfAPuBb4DNgN2+xn1j9a/89tdVtdqyUMlfjiSvMFro1X2fbwdlC67sAcBtwN/DhVFa32XjzXfsKjyYewBLwAXAYmAE+Bp4GjgJXtnk8foPn+hfwYJO/T9seztfZdvExzdmuPt+zwHzTv1cbbmp4qZRyESAi3gDuK6WcAw42u6zecL71ONt6pjLbiDgBPAc8Ov0lTqYN12y/WPfzD8DtTS2kp5xvPc62nj3PNiKOA28Cz5RS3pvWwnarDbHdJCKORsR32zyeaHqNXeZ863G29Uwy24g4BrwNvFBKebW5Vf+qDZcRNimlfM6Yr2QRcQuw9iGu+yLiVuDnsnqxRps533qcbT3jzjYijgDvAC+vXnpohVae2U7oE+BH4Ajw1urPxxpdUb8433qcbR1PAXcBz68/8216URNt/ZIk7U4fzmwlqfWMrSQlMLaSlMDYSlICYytJCSbaZzs7O1tGo1GlpbTf4uLi5VLpAz2crbOtpeZswfmOO9+JYjsajVhYWNj9qjouIqb9yVG/cLbOtpaas4V+zTcidvwzG7fLjjtfLyNIUgJjK0kJjK0kJTC2kpTA2EpSAmMrSQmMrSQlMLaSlKCV39QgSU2o+fnentlKUgLPbDVIu7ktU9oLz2wlKYGxlaQExlaSEhhbSUpgbCUpgbGVpATGVpISGFtJSuBNDRokb1hQNs9sJSmBsZWkBMZWkhIYW0lKYGwlKYGxlaQExlaSEhhbSUpgbCUpgbGVpATGVpISGFtJSmBsJWkb43wT8zj81C9JuoH1kV37eS+fFueZrSRtsNXZ7F7Oco2tJCUwtpK0zk5nr7s9uzW2krTOTtdld3vd1thKUgJjK0kbbHX22rndCNPatyZJtZRSfonr+p93Kz22a6E1uJK6YFrfxJwa242BNbiShiIttjU2CUtSV6TEtta+NU3GOUvNSYltrX1rGp/XyqVmpV1GqLGVQuPxWrnUvNQ3yDaG1dDW57VyqR3St36t37emurxWLrVHIzc1GNocXiuX2sPbdXvOa+VSOxjbAfBaudQ8YzsQXiuXmmVsB8TQSs0xtpKUwNhKUgJjK0kJjK0kJTC2kpTA2EpSgk7Hdn5+npMnTzIzM8Pc3BynT5/m0qVLTS+rN5xvPRcuXOD+++/n0KFDHDp0iIceeogLFy40vaxeaOtx2+nYrqyscPbsWZaWllheXubAgQOcOXOm6WX1hvOt5/Dhw5w/f56vv/6ay5cv88gjj/DYY481vaxeaOtxG5NsdI+Ir4DlKf797wW+BH4D7AO+BT4Ddrv7/jbgbuDDqaxus2OllLkaT1xhttCt+Q59tnPAb+nYbMEuMO58176it4kHsAR8ABwGZoCPgaeBo8CVbR6Pb/F8zwLzTf5ObXo43/bPdvW/+w/wP+DPTf9ebXj09bi9ecca1/dSKeUiQES8AdxXSjkHHJzkSSLiBPAc8Oj0l9hpzreePc+2lHIwIvYDTzL9s+8u691x24Zrtl+s+/kH4PZJnyAijgNvAs+UUt6b1sJ6wvnWs+fZApRSvgfOAa9ExB3TWFgP9O64bUNsN4mIoxHx3TaPJ9b92WPA28ALpZRXm1t1dzjfeiaZ7QY3ce3a4pHE5XZK14/bNlxG2KSU8jljvJJFxBHgHeDl1X/F0Bicbz0TzPYkcBn4CNgPvAiscO36pG6g68dtK89sJ/AUcBfw/PpXuKYX1SPOt56DwOvAN8CnwHHg4VLKT42uqh9aedxOtPVLkrQ7XT+zlaROMLaSlMDYSlICYytJCSba+jU7O1tGo1GlpbTf4uLi5VLpHnNn62xrqTlbcL7jznei2I5GIxYWFna/qo6LiGq3U9aYbUTQld0mXZttl9ScLTjfcefrZYSeiojr/lNSs4xtD20MrMGVmmdse2arsBpcqVnGtkd2CqrBlZpjbHtkpzfDuvJmmdRHxrZntgqqoZWaZWx7aGNYDa3UPGPbU2uBNbRSOxjbHjO0UnsYW0lKYGwl3Ban+lr5HWRSpvW3NnvpZW/GedEa6ow9s9WgeWuzshhbDZa3NiuTsdUgeWuzshlbDZK3NiubsdVgeWuzMhlbDZq3NiuLsdXgeWuzMhhbCUOr+rypQdLUdOlFK/sGDM9spXXc8qVajK20ym8kVk3GVsLbdlWfsdXgeduuMhhbDZq37SqLsdWgeduushhbDZ637SqD+2wlroV1/SWDmqH1A7aHydhKq9aCa+iGIfv/Zy8jSOsYWtVibCUpgbGVpATGVpISGFtJSjCY2HonkKQmDSK2fpqTpKb1fp/tjT7Nye09uXyRu57H3zD1+szWT3OS1Ba9ja2f5iSpTXobWz/NSVKb9Da24Kc5SWqPXscWNofV0EpqQu9jC78G1tBKasogYguGVlKzBhNbSWpS729q2Io3N+QZZ85uxVPfDfLM1tt3JWUbXGxvdPuuJNU2qNh6+66kpgwmtt6+K6lJg4mtt+9KatJgYgveviupOYOKLXj7rqRmDC624O27kvINMrZgaCXlGmxsJSmTsZWkBMZWkhIYW0lKYGwlKYGxlaQEMckWqIj4Cliut5zWO1ZKmavxxM7W2VZUbbbgfBlzvhPFVpK0O15GkKQExlaSEhhbSUpgbCUpgbGVpATGVpISGFtJSmBsJSmBsZWkBP8H0kUL3Jx4XDQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_layer1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_layer2 (Conv2D)       (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "maxpooling2d_layer1 (MaxPool (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_layer1 (Dropout)     (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer1 (Flatten)     (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_layer2 (Dropout)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 713s 17ms/step - loss: 0.6292 - acc: 0.7221 - val_loss: 0.9297 - val_acc: 0.5092\n",
      "Epoch 2/5\n",
      "28200/42000 [===================>..........] - ETA: 2:34 - loss: 0.1044 - acc: 0.9750"
     ]
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "diff_shape_same_radius_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_diff_shapes_same_radius.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save(diff_shape_same_radius_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
