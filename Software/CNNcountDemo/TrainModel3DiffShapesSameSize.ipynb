{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model3 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes with same size/radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically all different shapes with the same size/radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from CNNcount import shape_images as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed\n",
    "diff_shapes_same_radius_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/diff_shapes_same_radius_60k.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train, test and validaiton subsets\n",
    "if os.path.isfile(diff_shapes_same_radius_fname): # already generated- just load\n",
    "    print (\"The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = si.load_split_data(diff_shapes_same_radius_fname)    \n",
    "    \n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of different shapes (circle, square, diamond) with same radius does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_layer1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_layer2 (Conv2D)       (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "maxpooling2d_layer1 (MaxPool (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_layer1 (Dropout)     (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer1 (Flatten)     (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_layer2 (Dropout)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 558s 13ms/step - loss: 0.6386 - acc: 0.7241 - val_loss: 0.0718 - val_acc: 0.9978\n",
      "Epoch 2/5\n",
      "15200/42000 [=========>....................] - ETA: 5:48 - loss: 0.1252 - acc: 0.9638"
     ]
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "diff_shape_same_radius_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_diff_shapes_same_radius.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save(diff_shape_same_radius_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
