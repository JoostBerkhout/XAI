{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model1 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circular shapes with the same radii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically circle shapes with the same radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if(K.tensorflow_backend):\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from CNNcount import shape_images as si\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed)\n",
    "same_shape_same_radius_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/circles_same_radius_60k.npz\"\n",
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the same shape (circle) with same radius already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train and validaiton subsets\n",
    "if os.path.isfile(same_shape_same_radius_fname): # already generated- just load\n",
    "    print (\"The file containing images of the same shape (circle) with same radius already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = \\\n",
    "                                                                si.load_split_data(same_shape_same_radius_fname)\n",
    "    \n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of the same shape (circle) with same radius does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") \n",
    "    \n",
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)\n",
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_layer1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_layer2 (Conv2D)       (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "maxpooling2d_layer1 (MaxPool (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_layer1 (Dropout)     (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer1 (Flatten)     (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_layer2 (Dropout)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[200,62,62,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node conv2d_layer1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_conv2d_layer1_input_0_0, conv2d_layer1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9ff866464854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_cnncount_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/eStep/XAI/Software/CNNcountDemo/CNNcount/model_count_shapes.py\u001b[0m in \u001b[0;36mtrain_cnncount_model\u001b[0;34m(model, images_train, labels_train, images_val, labels_val, batch_size, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m               validation_data=(images_val, labels_val))\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[200,62,62,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node conv2d_layer1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_conv2d_layer1_input_0_0, conv2d_layer1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "same_shape_same_radius_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_circles_same_radius.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save(same_shape_same_radius_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
