{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model3 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes with same size/radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically all different shapes with the same size/radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from CNNcount import shape_images as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed\n",
    "diff_shapes_same_radius_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/diff_shapes_same_radius_60k.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train, test and validaiton subsets\n",
    "if os.path.isfile(diff_shapes_same_radius_fname): # already generated- just load\n",
    "    print (\"The file containing images of the different shapes (circle, diamond, square) with same radius already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = si.load_split_data(diff_shapes_same_radius_fname)    \n",
    "    \n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of different shapes (circle, square, diamond) with same radius does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elena/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 374s 9ms/step - loss: 0.6270 - acc: 0.7174 - val_loss: 0.1770 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      " 3600/42000 [=>............................] - ETA: 5:30 - loss: 0.1458 - acc: 0.9694"
     ]
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "diff_shape_same_radius_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_diff_shapes_same_radius.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save(diff_shape_same_radius_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
