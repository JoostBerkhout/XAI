{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model4 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes with different size/radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically all different shapes with different size/radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if(K.tensorflow_backend):\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from CNNcount import shape_images as si\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed\n",
    "diff_shapes_diff_radii_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/diff_shapes_diff_radii_60k.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the different shapes (circle, diamond, square) with different radii already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train, test and validaiton subsets\n",
    "if os.path.isfile(diff_shapes_diff_radii_fname): # already generated- just load\n",
    "    print (\"The file containing images of the different shapes (circle, diamond, square) with different radii already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = si.load_split_data(diff_shapes_diff_radii_fname)    \n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of different shapes (circle, square, diamond) with different radii does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_layer1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_layer2 (Conv2D)       (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "maxpooling2d_layer1 (MaxPool (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_layer1 (Dropout)     (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer1 (Flatten)     (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_layer2 (Dropout)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/8\n",
      "42000/42000 [==============================] - 369s 9ms/step - loss: 0.7880 - acc: 0.6225 - val_loss: 0.4286 - val_acc: 0.8429\n",
      "Epoch 2/8\n",
      "42000/42000 [==============================] - 374s 9ms/step - loss: 0.4617 - acc: 0.7939 - val_loss: 0.5967 - val_acc: 0.7098\n",
      "Epoch 3/8\n",
      "42000/42000 [==============================] - 376s 9ms/step - loss: 0.3186 - acc: 0.8646 - val_loss: 0.1696 - val_acc: 0.9435\n",
      "Epoch 4/8\n",
      "42000/42000 [==============================] - 377s 9ms/step - loss: 0.2230 - acc: 0.9104 - val_loss: 0.1162 - val_acc: 0.9595\n",
      "Epoch 5/8\n",
      "42000/42000 [==============================] - 366s 9ms/step - loss: 0.1469 - acc: 0.9432 - val_loss: 0.0936 - val_acc: 0.9709\n",
      "Epoch 6/8\n",
      "42000/42000 [==============================] - 358s 9ms/step - loss: 0.0978 - acc: 0.9650 - val_loss: 0.0504 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "42000/42000 [==============================] - 339s 8ms/step - loss: 0.0745 - acc: 0.9749 - val_loss: 0.0354 - val_acc: 0.9885\n",
      "Epoch 8/8\n",
      "42000/42000 [==============================] - 347s 8ms/step - loss: 0.0779 - acc: 0.9753 - val_loss: 0.0322 - val_acc: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc6f13e16d8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "diff_shape_diff_radii_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_diff_shapes_diff_radii.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save the trained model\n",
    "model.save(diff_shape_diff_radii_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
