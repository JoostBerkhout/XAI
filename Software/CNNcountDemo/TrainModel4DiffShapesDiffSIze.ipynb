{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model4 for counting shapes in binary images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes with different size/radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a CNN model for the Counting simple shapes (circles, squares or diamonds) experiment, more specifically all different shapes with different size/radius. The 'CNNcount' code is in a [git repository](https://github.com/NLeSC/XAI/tree/master/Software/CNNcountDemo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if(K.tensorflow_backend):\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from CNNcount import shape_images as si\n",
    "from CNNcount import model_count_shapes as mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for loading the data from the NPZ files (NumPy compressed\n",
    "diff_shapes_diff_radii_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/diff_shapes_diff_radii_60k.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-generated data and formatting of the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions and number of classes\n",
    "img_rows, img_cols = 64, 64\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file containing images of the different shapes (circle, diamond, square) with different radii already exist!\n",
      "Size of training data:  (42000, 64, 64, 1) and labels:  (42000,)\n",
      "Size of validation data:  (12000, 64, 64, 1) and labels:  (12000,)\n"
     ]
    }
   ],
   "source": [
    "# load the set of NIM images with the same type and same radius and split to train, test and validaiton subsets\n",
    "if os.path.isfile(diff_shapes_diff_radii_fname): # already generated- just load\n",
    "    print (\"The file containing images of the different shapes (circle, diamond, square) with different radii already exist!\")\n",
    "    # load from NPZ file for display\n",
    "    images_train, images_val, _, labels_train, labels_val, _ = si.load_split_data(diff_shapes_diff_radii_fname)    \n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "        images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "    print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "else: # missing data\n",
    "    print (\"The file containing images of different shapes (circle, square, diamond) with different radii does not exist!\")\n",
    "    print(\"Use the GenerateShapeImages notebook to generate the experimental data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot random 12 of the train images\n",
    "si.plot_12images(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "labels_train = np_utils.to_categorical(labels_train-1, num_classes=None)\n",
    "labels_val = np_utils.to_categorical(labels_val-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "labels_train shape: (42000, 3)\n",
      "labels_val shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('labels_val shape:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the training\n",
    "batch_size = 200\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_layer1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_layer2 (Conv2D)       (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "maxpooling2d_layer1 (MaxPool (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_layer1 (Dropout)     (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer1 (Flatten)     (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer1 (Dense)         (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_layer2 (Dropout)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer2 (Dense)         (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,392,131\n",
      "Trainable params: 7,392,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate the model\n",
    "model = mcs.generate_cnncount_model(input_shape, num_classes)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 12000 samples\n",
      "Epoch 1/8\n",
      "42000/42000 [==============================] - 364s 9ms/step - loss: 0.7728 - acc: 0.6255 - val_loss: 0.7125 - val_acc: 0.6307\n",
      "Epoch 2/8\n",
      "42000/42000 [==============================] - 361s 9ms/step - loss: 0.4690 - acc: 0.7861 - val_loss: 0.2777 - val_acc: 0.9007\n",
      "Epoch 3/8\n",
      "42000/42000 [==============================] - 360s 9ms/step - loss: 0.3198 - acc: 0.8639 - val_loss: 0.1848 - val_acc: 0.9340\n",
      "Epoch 4/8\n",
      "42000/42000 [==============================] - 361s 9ms/step - loss: 0.2241 - acc: 0.9095 - val_loss: 0.1578 - val_acc: 0.9402\n",
      "Epoch 5/8\n",
      "42000/42000 [==============================] - 359s 9ms/step - loss: 0.1619 - acc: 0.9370 - val_loss: 0.0915 - val_acc: 0.9694\n",
      "Epoch 6/8\n",
      "42000/42000 [==============================] - 346s 8ms/step - loss: 0.1056 - acc: 0.9610 - val_loss: 0.0596 - val_acc: 0.9817\n",
      "Epoch 7/8\n",
      "42000/42000 [==============================] - 5565s 132ms/step - loss: 0.0704 - acc: 0.9760 - val_loss: 0.0470 - val_acc: 0.9853\n",
      "Epoch 8/8\n",
      "42000/42000 [==============================] - 346s 8ms/step - loss: 0.0524 - acc: 0.9822 - val_loss: 0.0352 - val_acc: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fa7a116d4e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train \n",
    "mcs.train_cnncount_model(model, images_train, labels_train,images_val, labels_val, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model saving\n",
    "diff_shape_diff_radii_model_fname = \"/home/elena/eStep/XAI/Data/CountingShapes/model_diff_shapes_diff_radii.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save the trained model\n",
    "model.save(diff_shape_diff_radii_model_fname)\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
